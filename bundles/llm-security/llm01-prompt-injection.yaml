rules:
  - id: llm01-prompt-injection-basic
    message: "Potential prompt injection vulnerability: Direct user input passed to LLM without validation"
    languages: [python, javascript, typescript]
    pattern: |
      $LLM.chat.completions.create(
        ...,
        messages=[
          ...,
          {
            "role": "user",
            "content": $USER_INPUT
          },
          ...
        ],
        ...
      )
    severity: HIGH
    metadata:
      cwe: "CWE-77"
      owasp-llm: "LLM01"
      category: "prompt-injection"
      description: "Detects direct user input being passed to LLM APIs without sanitization"

  - id: llm01-prompt-injection-template
    message: "Potential prompt injection: String interpolation in prompt templates"
    languages: [python, javascript, typescript]
    pattern: |
      $TEMPLATE = f"...{$USER_VAR}..."
    severity: MEDIUM
    metadata:
      cwe: "CWE-77"
      owasp-llm: "LLM01"
      category: "prompt-injection"
      description: "Detects template string interpolation that could lead to prompt injection"

  - id: llm01-untrusted-prompt-concat
    message: "Potential prompt injection: Untrusted input concatenated with prompts"
    languages: [python, javascript, typescript]
    pattern: |
      $PROMPT = $BASE_PROMPT + $USER_INPUT
    severity: HIGH
    metadata:
      cwe: "CWE-77"
      owasp-llm: "LLM01"
      category: "prompt-injection"
      description: "Detects string concatenation with untrusted user input in prompts"
