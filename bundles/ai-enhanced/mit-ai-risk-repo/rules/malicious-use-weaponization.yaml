version: "1.0"
id: "tavoai-mit-malicious-weaponization"
name: "MIT AI Risk: AI Weaponization"
category: "security"
subcategory: "malicious-use"
severity: "critical"
rule_type: "hybrid"

standards:
  cwe: ["CWE-284", "CWE-502"]
  capec: ["CAPEC-550", "CAPEC-137"]
  owasp_llm: ["LLM01"]
  iso_42001: ["7.5.1"]
  nist_ai_rmf: ["PROTECT-3.1"]
  mit_ai_risk: ["Malicious Use"]

compatible_models:
  - "openai/gpt-4"
  - "anthropic/claude-3-opus"

tags:
  - "mit-ai-risk"
  - "malicious-use"
  - "weaponization"

heuristics:
  - type: "semgrep"
    languages: ["python", "javascript", "typescript"]
    pattern: |
      # Detect autonomous weapon-like capabilities
      def autonomous_attack($TARGET):
          while true:
              damage = calculate_damage($TARGET)
              execute_attack($TARGET, damage)
    message: "Autonomous attack capabilities without human oversight"
    severity: "critical"

ai_analysis:
  trigger:
    - "always"
  high_risk_patterns:
    - "*/weapon/*"
    - "*/attack/*"
    - "*/autonomous/*"
    
  prompt_template: |
    Analyze for AI weaponization risks and autonomous attack capabilities.
    Code: {code_snippet}
    Evaluate potential for creating autonomous weapons or attack systems.

  expected_response_schema:
    type: "object"
    required: ["severity", "vulnerable_lines", "description", "remediation", "standards_mapping", "confidence"]
    properties:
      severity: { enum: ["low", "medium", "high", "critical"] }
      vulnerable_lines: { type: "array", items: { type: "number" } }
      description: { type: "string", minLength: 100 }
      remediation: { type: "string", minLength: 100 }
      standards_mapping: { type: "object", properties: { mit_ai_risk: { type: "array", items: { type: "string" } } } }
      confidence: { type: "number", minimum: 0, maximum: 1 }

execution:
  max_tokens: 2000
  temperature: 0.1

sarif_output:
  rule_id: "tavoai-mit-malicious-weaponization"
  rule_name: "MIT AI Risk: AI Weaponization"
  short_description: "Detects AI systems that could be weaponized"
  full_description: "Evaluates AI systems for potential weaponization and autonomous attack capabilities"
  help_uri: "https://docs.tavoai.com/rules/mit-malicious-weaponization"
  tags: ["security", "mit-ai-risk", "malicious-use"]
EOF && cat > malicious-use-deception-manipulation.yaml << 'EOF'
version: "1.0"
id: "tavoai-mit-malicious-deception-manipulation"
name: "MIT AI Risk: Deception and Manipulation"
category: "ethics"
subcategory: "malicious-use"
severity: "high"
rule_type: "hybrid"

standards:
  cwe: ["CWE-1395", "CWE-502"]
  capec: ["CAPEC-151", "CAPEC-137"]
  owasp_llm: ["LLM01"]
  iso_42001: ["6.3.1"]
  nist_ai_rmf: ["VALIDATE-2.3"]
  mit_ai_risk: ["Malicious Use"]

compatible_models:
  - "openai/gpt-4"
  - "anthropic/claude-3-opus"

tags:
  - "mit-ai-risk"
  - "malicious-use"
  - "deception"

heuristics:
  - type: "semgrep"
    languages: ["python", "javascript", "typescript"]
    pattern: |
      # Detect deepfake or synthetic media generation
      def generate_fake_video($REAL_VIDEO, $TARGET_FACE):
          # Swap faces or manipulate content
          return manipulate_video($REAL_VIDEO, $TARGET_FACE)
    message: "Synthetic media generation capabilities"
    severity: "high"

ai_analysis:
  trigger:
    - "always"
  high_risk_patterns:
    - "*/synthetic/*"
    - "*/deepfake/*"
    - "*/manipulation/*"
    
  prompt_template: |
    Analyze for AI-powered deception and manipulation capabilities.
    Code: {code_snippet}
    Evaluate potential for creating convincing synthetic media or manipulation tools.

  expected_response_schema:
    type: "object"
    required: ["severity", "vulnerable_lines", "description", "remediation", "standards_mapping", "confidence"]
    properties:
      severity: { enum: ["low", "medium", "high", "critical"] }
      vulnerable_lines: { type: "array", items: { type: "number" } }
      description: { type: "string", minLength: 100 }
      remediation: { type: "string", minLength: 100 }
      standards_mapping: { type: "object", properties: { mit_ai_risk: { type: "array", items: { type: "string" } } } }
      confidence: { type: "number", minimum: 0, maximum: 1 }

execution:
  max_tokens: 2000
  temperature: 0.1

sarif_output:
  rule_id: "tavoai-mit-malicious-deception-manipulation"
  rule_name: "MIT AI Risk: Deception and Manipulation"
  short_description: "Detects AI systems enabling deception"
  full_description: "Evaluates AI capabilities for creating deceptive content or manipulating information"
  help_uri: "https://docs.tavoai.com/rules/mit-malicious-deception-manipulation"
  tags: ["ethics", "mit-ai-risk", "malicious-use"]
