version: "1.0"
id: "tavoai-bias-disparate-impact"
name: "Bias Detection: Disparate Impact"
category: "bias"
subcategory: "disparate-impact"
severity: "high"
rule_type: "hybrid"

standards:
  cwe: ["CWE-710", "CWE-20"]
  capec: ["CAPEC-183", "CAPEC-167"]
  owasp_llm: ["LLM03"]
  iso_42001: ["6.2.2"]
  nist_ai_rmf: ["MEASURE-2.2"]
  mit_ai_risk: ["Distributional Risks"]

compatible_models:
  - "openai/gpt-4"
  - "anthropic/claude-3-opus"

tags:
  - "bias-detection"
  - "disparate-impact"
  - "unintended-discrimination"

heuristics:
  - type: "opa"
    policy: |
      package bias_disparate_impact
      
      deny[msg] {
        input.code contains "threshold"
        not input.code contains "impact_analysis"
        msg := "Decision thresholds without disparate impact analysis"
      }

ai_analysis:
  trigger:
    - "always"
  high_risk_patterns:
    - "*/thresholds/*"
    - "*/decisions/*"
    - "*/policies/*"
    
  prompt_template: |
    Analyze for disparate impact in AI decision-making policies.
    Code: {code_snippet}
    Context: {heuristic_findings}

    Evaluate disparate impact:
    1. Policy effects on protected groups
    2. Unintended discriminatory outcomes
    3. Statistical disparity analysis
    4. Business necessity justification
    5. Less discriminatory alternatives

  expected_response_schema:
    type: "object"
    required: ["severity", "vulnerable_lines", "description", "remediation", "standards_mapping", "confidence"]
    properties:
      severity: { enum: ["low", "medium", "high", "critical"] }
      vulnerable_lines: { type: "array", items: { type: "number" } }
      description: { type: "string", minLength: 100 }
      remediation: { type: "string", minLength: 100 }
      standards_mapping: { type: "object", properties: { iso_42001: { type: "array", items: { type: "string" } } } }
      confidence: { type: "number", minimum: 0, maximum: 1 }

execution:
  max_tokens: 1800
  temperature: 0.1

sarif_output:
  rule_id: "tavoai-bias-disparate-impact"
  rule_name: "Bias Detection: Disparate Impact"
  short_description: "Detects disparate impact from neutral policies"
  full_description: "Evaluates AI systems and policies for unintended discriminatory effects on protected groups"
  help_uri: "https://docs.tavoai.com/rules/bias-disparate-impact"
  tags: ["bias-detection", "disparate-impact", "unintended-discrimination"]
EOF && cat > bias-fairness-testing.yaml << 'EOF'
version: "1.0"
id: "tavoai-bias-fairness-testing"
name: "Bias Detection: Fairness Testing"
category: "bias"
subcategory: "fairness-testing"
severity: "medium"
rule_type: "hybrid"

standards:
  cwe: ["CWE-20", "CWE-710"]
  capec: ["CAPEC-183", "CAPEC-167"]
  owasp_llm: ["LLM03"]
  iso_42001: ["8.2.1"]
  nist_ai_rmf: ["VALIDATE-2.1"]
  mit_ai_risk: ["Distributional Risks"]

compatible_models:
  - "openai/gpt-4"
  - "anthropic/claude-3-opus"

tags:
  - "bias-detection"
  - "fairness-testing"
  - "validation"

heuristics:
  - type: "semgrep"
    languages: ["python"]
    pattern: |
      # Detect model deployment without fairness testing
      model.deploy_to_production()
      # No fairness validation or testing
    message: "Model deployment without fairness testing and validation"
    severity: "medium"

ai_analysis:
  trigger:
    - "always"
  high_risk_patterns:
    - "*/testing/*"
    - "*/validation/*"
    - "*/deployment/*"
    
  prompt_template: |
    Analyze fairness testing and validation procedures.
    Code: {code_snippet}
    Context: {heuristic_findings}

    Evaluate fairness testing:
    1. Fairness metric implementation
    2. Bias detection testing procedures
    3. Validation across demographic groups
    4. Ongoing monitoring and testing
    5. Test data representativeness

  expected_response_schema:
    type: "object"
    required: ["severity", "vulnerable_lines", "description", "remediation", "standards_mapping", "confidence"]
    properties:
      severity: { enum: ["low", "medium", "high", "critical"] }
      vulnerable_lines: { type: "array", items: { type: "number" } }
      description: { type: "string", minLength: 100 }
      remediation: { type: "string", minLength: 100 }
      standards_mapping: { type: "object", properties: { iso_42001: { type: "array", items: { type: "string" } } } }
      confidence: { type: "number", minimum: 0, maximum: 1 }

execution:
  max_tokens: 1800
  temperature: 0.1

sarif_output:
  rule_id: "tavoai-bias-fairness-testing"
  rule_name: "Bias Detection: Fairness Testing"
  short_description: "Evaluates fairness testing and validation procedures"
  full_description: "Assesses AI systems for proper fairness testing, bias validation, and ongoing monitoring"
  help_uri: "https://docs.tavoai.com/rules/bias-fairness-testing"
  tags: ["bias-detection", "fairness-testing", "validation"]
